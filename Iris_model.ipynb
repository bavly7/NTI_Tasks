{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2861a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491cfe21",
   "metadata": {},
   "source": [
    "## Load data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f64542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features: Sepal length, Sepal width, Petal length, Petal width\n",
    "y = iris.target.reshape(-1, 1)  # Labels as column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931e4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b6ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b8e14",
   "metadata": {},
   "source": [
    "## One-hot encode labels (3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647347dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5c7096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e994b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9bad878",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30cf93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):  \n",
    "    return 1 / (1 + np.exp(-z))  # σ(z) = 1 / (1 + e^(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0cca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(a):  \n",
    "    return a * (1 - a)  # σ'(z) = σ(z) * (1 - σ(z)) its also y^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5538a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):  \n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical Stability trick\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # softmax (sig / summation of all sigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fc810fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):  \n",
    "    m = y_true.shape[0]  # number of samples\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-9)) / m  # L = -(1/m) Σ y_true * log(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c19bc24",
   "metadata": {},
   "source": [
    "## Inputs:\n",
    "\n",
    "- x → shape (m, 4) (m samples, 4 features each)\n",
    "\n",
    "- W1 → shape (4, 8) (weights input → hidden)\n",
    "\n",
    "- b1 → shape (1, 8) (bias hidden layer)\n",
    "\n",
    "- W2 → shape (8, 3) (weights hidden → output)\n",
    "\n",
    "- b2 → shape (1, 3) (bias output layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e979a9",
   "metadata": {},
   "source": [
    "| Variable | Shape    | Meaning                                                               |\n",
    "| -------- | -------- | --------------------------------------------------------------------- |\n",
    "| **X**    | `(m, 4)` | Input features (m samples, 4 features per sample).                    |\n",
    "| **W1**   | `(4, 8)` | Weights from input → hidden layer. Each column is a neuron’s weights. |\n",
    "| **b1**   | `(1, 8)` | Bias for each hidden neuron.                                          |\n",
    "| **Z1**   | `(m, 8)` | Weighted sum going into hidden neurons: `Z1 = X·W1 + b1`.             |\n",
    "| **A1**   | `(m, 8)` | Activation output of hidden neurons: `A1 = sigmoid(Z1)`.              |\n",
    "| **W2**   | `(8, 3)` | Weights from hidden → output layer.                                   |\n",
    "| **b2**   | `(1, 3)` | Bias for each output neuron.                                          |\n",
    "| **Z2**   | `(m, 3)` | Weighted sum going into output neurons: `Z2 = A1·W2 + b2`.            |\n",
    "| **A2**   | `(m, 3)` | Final output probabilities: `A2 = softmax(Z2)`.                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a188c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    # Hidden layer\n",
    "    Z1 = np.dot(X, W1) + b1           # Rule: Z1 = X · W1 + b1\n",
    "    A1 = sigmoid(Z1)                  # Rule: A1 = σ(Z1)\n",
    "\n",
    "    # Output layer\n",
    "    Z2 = np.dot(A1, W2) + b2           # Rule: Z2 = A1 · W2 + b2\n",
    "    A2 = softmax(Z2)                   # Rule: A2 = softmax(Z2)\n",
    "\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a61b29",
   "metadata": {},
   "source": [
    "## Outputs:\n",
    "\n",
    "- Z1 → shape (m, 8)\n",
    "\n",
    "- A1 → shape (m, 8)\n",
    "\n",
    "- Z2 → shape (m, 3)\n",
    "\n",
    "- A2 → shape (m, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1adafe0",
   "metadata": {},
   "source": [
    "| Variable    | Shape    | Meaning                                                                                        |\n",
    "| ----------- | -------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **y\\_true** | `(m, 3)` | One-hot encoded labels for each sample.                                                        |\n",
    "| **dZ2**     | `(m, 3)` | Error signal at output layer: `A2 - y_true`. This is how far predictions are from true labels. |\n",
    "| **dW2**     | `(8, 3)` | Gradient of loss wrt W2: how much each hidden→output weight should change.                     |\n",
    "| **db2**     | `(1, 3)` | Gradient of loss wrt b2: how much each output neuron’s bias should change.                     |\n",
    "| **dA1**     | `(m, 8)` | Error signal propagated backwards into hidden layer: `dZ2·W2.T`.                               |\n",
    "| **dZ1**     | `(m, 8)` | Error signal at hidden layer after activation derivative: `dA1 * sigmoid_derivative(A1)`.      |\n",
    "| **dW1**     | `(4, 8)` | Gradient of loss wrt W1: how much each input→hidden weight should change.                      |\n",
    "| **db1**     | `(1, 8)` | Gradient of loss wrt b1: how much each hidden neuron’s bias should change.                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcb45b",
   "metadata": {},
   "source": [
    "## Inputs:\n",
    "\n",
    "- X → shape (m, 4)\n",
    "\n",
    "- y_true → shape (m, 3) (one-hot encoded labels)\n",
    "\n",
    "- Z1, A1, A2 (from forward pass)\n",
    "\n",
    "- W2 → shape (8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e136564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, y, Z1, A1, A2, W2):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Output layer error\n",
    "    dZ2 = A2 - y                      # Rule: ∂L/∂Z2 = A2 - y\n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)   # Rule: ∂L/∂W2 = (1/m) · A1^T · dZ2\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)  # Rule: ∂L/∂b2 = (1/m) Σ dZ2\n",
    "\n",
    "    # Hidden layer error\n",
    "    dA1 = np.dot(dZ2, W2.T)           # Rule: ∂L/∂A1 = dZ2 · W2^T\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)  # Rule: ∂L/∂Z1 = ∂L/∂A1 ⊙ σ'(Z1)\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)    # Rule: ∂L/∂W1 = (1/m) · X^T · dZ1\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)  # Rule: ∂L/∂b1 = (1/m) Σ dZ1\n",
    "\n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1e40b",
   "metadata": {},
   "source": [
    "## Outputs:\n",
    "\n",
    "- dW1 → (4, 8)\n",
    "\n",
    "- db1 → (1, 8)\n",
    "\n",
    "- dW2 → (8, 3)\n",
    "\n",
    "- db2 → (1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05b871",
   "metadata": {},
   "source": [
    "Forward pass takes weights + X → returns activations & predictions.\n",
    "\n",
    "Backward pass takes activations + true labels → returns weight & bias gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aeeacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "input_size = X_train.shape[1]  # 4 features\n",
    "hidden_size = 8                # neurons in hidden layer\n",
    "output_size = y_train.shape[1] # 3 classes\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f29cbef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 1.0981\n",
      "Epoch 100, Loss: 1.0952\n",
      "Epoch 150, Loss: 1.0691\n",
      "Epoch 200, Loss: 0.9253\n",
      "Epoch 250, Loss: 0.6961\n",
      "Epoch 300, Loss: 0.5580\n",
      "Epoch 350, Loss: 0.4831\n",
      "Epoch 400, Loss: 0.4336\n",
      "Epoch 450, Loss: 0.3958\n",
      "Epoch 500, Loss: 0.3645\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1, A1, Z2, A2 = forward_pass(X_train, W1, b1, W2, b2)\n",
    "\n",
    "    # Loss\n",
    "    loss = cross_entropy_loss(y_train, A2)\n",
    "\n",
    "    # Backward pass\n",
    "    dW1, db1, dW2, db2 = backward_pass(X_train, y_train, Z1, A1, A2, W2)\n",
    "\n",
    "    # Update parameters (Gradient Descent Rule)\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, A1_test, _, A2_test = forward_pass(X_test, W1, b1, W2, b2)\n",
    "predictions = np.argmax(A2_test, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df9a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
